---
title: "Genotype Data Prep"
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_float: true
    toc_collapsed: false
---

```{r, message=FALSE, warning=FALSE}
require(tidyverse)
require(magrittr)
```

# Readme

This is an R notebook. To view a rendered, interactive log of all work done open the .html version in a browser. To run or edit code yourself clone the parent github repository that contains this notebook, and open the R project in R studio. All data is available and paths should work correctly.

A lot of the work in this notebook is writing new files to disk. These code chunks have their eval flag set to false, to not overwrite files when rendering the notebook. All code chunks with an eval=FALSE flag need to be manually run in the console to write files.

# Summary

This notebook achieves several data prep goals for the McKenzie Genetic Pedigree Project:  

(1) Convert raw genemapper output from 2022 and data from previous reports into two cleaned up and consolidated datasets, long and wide, that can be used to prepare data for Cervus and Colony runs, plan re-runs and serve as a long term store of data with good documentation.  
(2) Plan which genotypes need re-runs.  
(3) Conduct additional, final QC check on which samples to include and explanation of any remaining discrepancies with previous publications  
(4) Prepare input datasets for Cervus and Colony.  

# Dataset Definitions
 
__gt_0.1__: no re-runs, no metadata, no previous data, just first round genotyping from 2022, used to plan re-runs. wide format  
__gt_l_0.1__: no re-runs, no metadata, no previous data, just first round genotyping from 2022, used to plan re-runs. long format  

__gt_0.3__: re-runs incorporated, no metadata, no previous data, no filtering of any kind. wide format   
__gt_l_0.3__: re-runs incorporated, no metadata, no previous data, no filtering of any kind. wide format  


# Data Import 

### Plates 17-20 Error

__Summary__  
The sample sheet uploaded to the ABI for the runs covering plates 17-20 was corrupted (see big mistake log, in many places in this repository). In short, panels 2 and 3 for plates 17-20 were output by the ABI as if they were the samples from plates 1-4. We can use a key from the 384 well id and sample names to correct this error. 

ALL datasets with the error are labeled "error." Corrected datasets are not labeled.

__Correction__  

We correct the error by renaming sample names in the genemapper output and saving to a new file (MCKR_Plate_17-20 Genotypes Table.txt). 
```{r, eval = FALSE}
gm_raw_17_20_error <- read_tsv("./genotype_data/genemapper_output/MCKR_Plate_17-20_error Genotypes Table.txt", col_types = cols(.default = "c"))

abi_input_error <- read_tsv("../lab_work/genotyping/ABI_input_sheets/input_sheets/012822_did_R2.txt", skip = 4)

abi_input_error %<>%
  select(Well, error_name = `Sample Name`)

key <- read_tsv("./genotype_data/genemapper_output/plates_17_20_correction_key.txt")

key %<>%
    filter(!(correct_name %in% c("positive", "negative"))) %>%
    left_join(abi_input_error)
  

gm_raw_17_20 <- gm_raw_17_20_error %>%
  left_join(key, by = c(`Sample Name` = "error_name")) %>%
  mutate(`Sample Name` = case_when(is.na(correct_name) ~ `Sample Name`,
                                   TRUE ~ correct_name)) %>%
  select(-c(correct_name, Well))


# let's do some checks
# each sample should have 12 rows now, except controls which occur multiple times
gm_raw_17_20 %>%
  group_by(`Sample Name`) %>%
  summarise(n = n()) %>%
  distinct(n, .keep_all = TRUE)

# YES checks out.

# There should also be 376 individuals + 2 controls
gm_raw_17_20 %>%
  distinct(`Sample Name`) %>%
  nrow()

# okay looks good, let's save this file
write_tsv(gm_raw_17_20, "./genotype_data/genemapper_output/MCKR_Plate_17-20 Genotypes Table.txt")

```


# Re-Run Planning

## gt_0.1 dataset
Let's import the first round of raw data from genemapper, clean up and consolidate into a single dataset (gt_0.1)

```{r, message=FALSE, warning=FALSE}
#import all raw data
gm_a <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_1-4 Genotypes Table.txt", col_types = cols(.default = "c"))

gm_b <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_5-8 Genotypes Table.txt", col_types = cols(.default = "c"))

gm_c <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_9-12 Genotypes Table.txt", col_types = cols(.default = "c"))

gm_d <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_13-16 Genotypes Table.txt", col_types = cols(.default = "c"))

gm_e <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_17-20 Genotypes Table.txt", col_types = cols(.default = "c"))

gm_f <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_21-24 Genotypes Table.txt", col_types = cols(.default = "c"))

gm_g <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_25-28 Genotypes Table.txt", col_types = cols(.default = "c"))

# consolidate
gm_all <- bind_rows(gm_a, gm_b, gm_c, gm_d, gm_e, gm_f, gm_g)


# remove controls, blanks, etc 
gm_all %<>%
  filter(str_starts(`Sample Name`, "Ots"))
```

Let's see how many scoring errors I left in the dataset. I'll check for more than two alleles, and unbinned alleles.

```{r}
# how many scoring errors did I miss (e.g. more than two alleles, unbinned alleles, etc),

gm_all %>%
  summarise(more_than_two_allele = sum(ADO == "true"), 
            unbinned_allele1 = sum(`Allele 1` == "?", na.rm = TRUE),
            unbinned_allele2 = sum(`Allele 2` == "?", na.rm = TRUE))


```

Out of 30960 individual genotype calls, I left 3 errors in the dataset, spread across 3 different genotypes (sometimes multiple errors occur at the same genotype). I don't think a 1 in 10k error rate is worth going back and fixing, but we should set these to NA.

```{r}
#clean up allele display overflow gts, set to NA
gm_all %<>%
  mutate(`Allele 1` = case_when(ADO == "true" ~ NA_character_,
                                TRUE ~ `Allele 1`),
         `Allele 2` = case_when(ADO == "true" ~ NA_character_,
                                TRUE ~ `Allele 2`),)

# remove all the individual objects
rm(gm_a)
rm(gm_b)
rm(gm_c)
rm(gm_d)
rm(gm_e)
rm(gm_f)
rm(gm_g)

```

Now let's prep the gt_0.1 objects

```{r}
# sorta long (each allele is its own column for each marker/individual)
gt_l_0.1 <- gm_all %>%
  select(sample = `Sample Name`, marker = Marker, panel = Panel, a1 = `Allele 1`, a2 = `Allele 2`)

# super long (for conversion only, totally long - each allele/marker/individual is its own row)
gt_0.1_sl <- gm_all %>%
  select(sample = `Sample Name`, marker = Marker, panel = Panel, a1 = `Allele 1`, a2 = `Allele 2`) %>%
  pivot_longer(cols = c(a1, a2), values_to = "allele")

#now convert to wide format (each marker gets two columns, format for colony and cervus)
gt_0.1 <- gt_0.1_sl %>%
  pivot_wider(id_cols = c(sample), names_from = c(marker, name), names_prefix = "Ot",  values_from = allele)
  

```

Nice this looks good. Now let's save the wide format dataset to work with later. 
```{r, eval = FALSE}
write_tsv(gt_0.1, "genotype_data/gt_0.1.txt")
```

## Missingness

Now we'll look at missingness and make decisions on re-runs.

### Missingness Rate

Only individuals with genotypes at >=7 loci will be included in the parentage analysis , a threshold that was determined based on the non-exclusion probabilities observed across loci for assigning parentage to an individual parent or parent pair.

As a bare minimum, we should try to get every individual over this barrier, and DPJ suggested this is the re-run genotyping approach used to arrive at the final dataset (re-run individuals with <=7 scored genotypes, once).

How many individuals need re-runs using this cutoff, what about stricter cutoffs

```{r}
missing_count <- gt_l_0.1 %>%
  filter(marker != 	"OtY3") %>%
  group_by(sample) %>%
  summarise(missing_gt = sum(is.na(a1)))

missing_count %>%
  summarise(n_at_least_7_loci = sum((11-missing_gt) <= 7),
            n_at_least_8_loci = sum((11-missing_gt) <= 8),
            n_at_least_9_loci = sum((11-missing_gt) <= 9))
```

Currently (no re-runs completed yet) 140 (~5%) of individuals would be filtered out of the final dataset. 
In previous reports, most missing data came from carcass samples. 6,079 of 6,119 reintroduced salmon were genotyped at at least 10 of 11 loci. Let's filter out our carcass samples and see how we are doing.

```{r}
missing_count_no_carcass <- gt_l_0.1 %>%
  filter(marker != 	"OtY3") %>%
  filter(str_detect(sample, "OtsAC")) %>%
  group_by(sample) %>%
  summarise(missing_gt = sum(is.na(a1)))

missing_count_no_carcass %>%
  summarise(n_at_least_7_loci = sum((11-missing_gt) <= 7),
            n_at_least_8_loci = sum((11-missing_gt) <= 8),
            n_at_least_9_loci = sum((11-missing_gt) <= 9))
```

Let's also look at the 116 carcass samples alone
```{r}
missing_count_carcass <- gt_l_0.1 %>%
  filter(marker != 	"OtY3") %>%
  filter(str_detect(sample, "OtsCC")) %>%
  group_by(sample) %>%
  summarise(missing_gt = sum(is.na(a1)))

missing_count_carcass %>%
  summarise(n_at_least_7_loci = sum((11-missing_gt) <= 7),
            n_at_least_8_loci = sum((11-missing_gt) <= 8),
            n_at_least_9_loci = sum((11-missing_gt) <= 9))
```

This dataset, even ignoring carcass samples, is not nearly as complete as the prior. While the 116 carcass samples are way overrepresented among poorly genotyped samples (represent about 1/3 of failed samples despite only representing about 5% of all samples), the non-carcass samples perform substantially worse than in the final dataset from the previous report. 

Will re-runs solve this problem? If most of the missing data comes from fish that failed at all or nearly all markers, it suggests something went wrong with the DNA extraction, not amplification. Re-runs starting from tissue could transition most of these samples with fewer than 10 scored gts to at least 10. Let's check:

First for non-carcass
```{r}
ggplot(data = missing_count_no_carcass)+geom_histogram(aes(x = missing_gt), bins =12)+theme_classic()
```

No, most individuals that would be excluded (greater than 3 missing genotypes), only failed at a few markers, not all. DNA quantiy/quantity was suficent for at least some of the markers and was likely not the problem. Let's also check how many completely failed (missing at at least 9 of 11 markers)

```{r}
sum(missing_count_no_carcass$missing_gt>=9)
```

Only 14 of the 2464 non-carcass samples failed to genotype at 9 or more (of 11) markers, suggesting there was at least some DNA of sufficient quantity/quantity to amplify.

Then for carcass:

```{r}
ggplot(data = missing_count_carcass)+geom_histogram(aes(x = missing_gt), bins =12)+theme_classic()
sum(missing_count_carcass$missing_gt>=9)
```

This suggests we would rarely benefit from extracting new DNA and that re-running an individual at all markers would produce mostly redudant information. Let's look a little more closely before committing to this.

If genotyping failure is not due to DNA quality it must stem from amplification or quantification. Are markers in the same sub-panel (amplified together) for an individual more likely to fail together than markers across panels in the same individual?

```{r}
# definitely a smarter way to do this, but lets just make 4 comparisons and average them all out

gt_0.1 %>%
  group_by(sample) %>%
  summarise(fail_within_253_249 = sum(is.na(Ot253b_a1) & is.na(Ot249_a1)),
            fail_without_253_311 = sum(is.na(Ot253b_a1) & is.na(Ot311_a1)),
            fail_within_201_209 =  sum(is.na(Ot201_a1) & is.na(Ot209_a1)),
            fail_without_201_212 =  sum(is.na(Ot201_a1) & is.na(Ot212_a1)),
            fail_within_208_211 = sum(is.na(Ot208_a1) & is.na(Ot211_a1)),
            fail_without_208_515 = sum(is.na(Ot208_a1) & is.na(Ot515_a1))) %>%
  summarise(fail_together_rate = (sum(fail_within_253_249)+sum(fail_within_201_209)+ sum(fail_within_208_211))/(2580*3), fail_apart_rate = (sum(fail_without_253_311)+sum(fail_without_201_212)+sum(fail_without_208_515))/(2580*3))
```

About twice as likely to fail together at the level of amplification as across panels. However with a 7% overall failure rate, if things were completely random outside of panels, we'd expect the fail apart rate to be around 0.5 percent. It's 1.8% so we can expect bad DNA contributes at least in part to failed samples.

Does DNA quality/quantity matter? We can use peak height as rough proxy for DNA quality/quantity. Does peak height at non-fail markers predict the number of failed markers?

```{r}
peak_height_data <- gm_all %>%
  filter(Marker != "OtY3") %>%
  group_by(`Sample Name`) %>%
  mutate(`Height 1` = as.numeric(`Height 1`)) %>%
  summarise(mean_height = mean(`Height 1`, na.rm = TRUE)) %>%
  rename(sample = `Sample Name`) %>%
  left_join(missing_count) %>%
  mutate(carcass = case_when(str_detect(sample, "OtsCC") ~ "carcass",
                             TRUE ~ "non_carcass"))


summary(glm(missing_gt ~ mean_height, data = peak_height_data, family = "poisson"))

ggplot(data = peak_height_data)+geom_smooth(aes(mean_height, missing_gt, color = carcass))+geom_point(aes(mean_height, missing_gt, color = carcass), alpha = 0.6)+theme_bw()+scale_colour_viridis_d(begin = 0.05, end = 0.8)

```

Wow, what a strong pattern. A clear inflection point of mean height around 3500, where it is likely that missing genotypes will increase. In contrast to the previous finding this suggests that DNA quality/quantity does drive missing data rates. 

How many individuals are below this cutoff?

```{r, message=FALSE}
gm_all %>%
  filter(Marker != "OtY3") %>%
  group_by(`Sample Name`) %>%
  mutate(`Height 1` = as.numeric(`Height 1`)) %>%
  summarise(mean_height = mean(`Height 1`, na.rm = TRUE)) %>%
  rename(sample = `Sample Name`) %>%
  left_join(missing_count) %>%
  filter(mean_height < 3000 ) %>%
  mutate(carcass = case_when(str_detect(sample, "OtsCC") ~ "carcass",
                             TRUE ~ "non_carcass")) %>%
  count(carcass)

#what about below the cutoff AND have below the cutoff for inclusion (missing >4 loci)
gm_all %>%
  filter(Marker != "OtY3") %>%
  group_by(`Sample Name`) %>%
  mutate(`Height 1` = as.numeric(`Height 1`)) %>%
  summarise(mean_height = mean(`Height 1`, na.rm = TRUE)) %>%
  rename(sample = `Sample Name`) %>%
  left_join(missing_count) %>%
  filter(mean_height < 3000 & missing_gt > 4 ) %>%
  mutate(carcass = case_when(str_detect(sample, "OtsCC") ~ "carcass",
                             TRUE ~ "non_carcass")) %>%
  count(carcass)


#what about below the cutoff AND are near the cutoff for inclusion (missing >3 loci)
gm_all %>%
  filter(Marker != "OtY3") %>%
  group_by(`Sample Name`) %>%
  mutate(`Height 1` = as.numeric(`Height 1`)) %>%
  summarise(mean_height = mean(`Height 1`, na.rm = TRUE)) %>%
  rename(sample = `Sample Name`) %>%
  left_join(missing_count) %>%
  filter(mean_height < 3000 & missing_gt > 3 ) %>%
  mutate(carcass = case_when(str_detect(sample, "OtsCC") ~ "carcass",
                             TRUE ~ "non_carcass")) %>%
  count(carcass)
```


This calls into question the best approach for re-runs. It suggests that there are some individuals that fail at some markers because of DNA quality.


### Conclusions

Some notes from above:  


(1) __DNA quality likely drives missingness__ While very few samples have such poor DNA quality/quantity that they fail at nearly all markers, there's a clear realtionship between peak height (a proxy for DNA quality/quantity) at markers that did not fail, and number of markers that did fail, with mean peak height of around 3000 as the inflection point. This suggests tht for many re-run samples, using the DNA on hand may not improve genotypes.  

(2) __Few individuals failed at all/most markers__ Most failed genotypes occured in individuals that other wise genotyped well. This suggests that there was at least some DNA that could be successfully amplified and quantified. 

(3) __Within Panel correlation__  Markers within a panel were more likely to fail together than markers across panels, suggesting that amplification was sometimes the problem.

There's mixed evidence that re-extraction is worth it (1 and 2 above conflict, and 3 is unclear if amplification itself was the problem or DNA quantity/quality used for amplification). Let's be conservative and try to extract DNA again.

Only 140 individuals failed to the point of NEEDING to be re-run to avoid filtering (>=4 missing loci /  7 successfully genotyped loci). So, at a minimum, we need to conduct re-runs at 2 96-well plates. Reducing our threshold for re-runs to >=3 missing loci is very close to two plates worth of individuals (198). If we focus on these individuals, the observation that running the same individuals across all panels would mostly be wasteful does not apply. Among these 198 individuals with 8 or fewer succesfully genotyped loci, most are missing at least a single genotype from each of the three panels.



## Plan

Let's keep planning

__Re-Extractions__  

About 90 individuals have 7 or fewer successfully scored gentoypes and have a mean peak height at the remaining markers below the inflection point at ~3000 units. Let's round this up to a full plate of extractions (94 samples).

```{r}
re_ext_inds <- gm_all %>%
  filter(Marker != "OtY3") %>%
  group_by(`Sample Name`) %>%
  mutate(`Height 1` = as.numeric(`Height 1`)) %>%
  summarise(mean_height = mean(`Height 1`, na.rm = TRUE)) %>%
  rename(sample = `Sample Name`) %>%
  left_join(missing_count) %>%
  filter(mean_height < 2310 & missing_gt > 3 ) %>% #adjusted these number until hit 94 samples
  mutate(carcass = case_when(str_detect(sample, "OtsCC") ~ "carcass",
                             TRUE ~ "non_carcass")) %>%
  pull(sample)
```

__Panel by Panel__

Should we do the worst single plate (94 individuals), or the worst 4x plate (376) per panel? Let's see what each would accomplish/require.

```{r}
missing_count_panel <- gt_l_0.1 %>%
  group_by(sample, panel) %>%
  summarise(missing_gt = sum(is.na(a1), na.rm = TRUE))

# exact number of individuals with x missing, per panel
missing_count_panel %>%
  ungroup() %>%
  group_by( panel) %>%
  summarise(miss_4_per_panel =  sum(missing_gt==4),
            miss_3_per_panel =  sum(missing_gt==3),
            miss_2_per_panel =  sum(missing_gt==2),
            miss_1_per_panel=  sum(missing_gt==1))
  
# number of missing individuals with x or more missing loci, per panel
missing_count_panel %>%
  ungroup() %>%
  group_by(panel) %>%
  summarise(miss_al4_per_panel =  sum(missing_gt>=4),
            miss_al3_per_panel =  sum(missing_gt>=3),
            miss_al2_per_panel =  sum(missing_gt>=2),
            miss_al1_per_panel=  sum(missing_gt>=1))
  #slice_max(order_by = missing_gt, n =376) %>%
  #count(panel)

```

If we do 376 per panel we can cover almost every missing locus in one set of re-runs if we use 384 plates. There are fewer than 376 individuals with any missing genotypes in panel 3 and the sex marker, and just over 376 in panels 1 and 2.

This seems like overkill. Let's tolerate 2 missing loci overall (at any panel). This leaves only 552 individuals total that need re-runs. What does this look like per panel?

```{r}
missing_at_least_2_overall <- missing_count %>%
  filter(missing_gt >=2) %>%
  pull(sample)

missing_count_panel_filtered <- gt_l_0.1 %>%
  filter(sample %in% missing_at_least_2_overall) %>%
  group_by(sample, panel) %>%
  summarise(missing_gt = sum(is.na(a1), na.rm = TRUE))

missing_count_panel_filtered %>%
  ungroup() %>%
  group_by(panel) %>%
  summarise(miss_al4_per_panel =  sum(missing_gt>=4),
            miss_al3_per_panel =  sum(missing_gt>=3),
            miss_al2_per_panel =  sum(missing_gt>=2),
            miss_al1_per_panel=  sum(missing_gt>=1))
```

 Let's also examine tolerating 3 missing loci overall (at any panel, at least 8 markers). This leaves only 198 individuals total that need re-runs. What does this look like per panel?

```{r}
missing_at_least_3_overall <- missing_count %>%
  filter(missing_gt >=3) %>%
  pull(sample)

missing_count_panel_filtered3 <- gt_l_0.1 %>%
  filter(sample %in% missing_at_least_3_overall) %>%
  group_by(sample, panel) %>%
  summarise(missing_gt = sum(is.na(a1), na.rm = TRUE))

missing_count_panel_filtered3 %>%
  ungroup() %>%
  group_by(panel) %>%
  summarise(miss_al4_per_panel =  sum(missing_gt>=4),
            miss_al3_per_panel =  sum(missing_gt>=3),
            miss_al2_per_panel =  sum(missing_gt>=2),
            miss_al1_per_panel=  sum(missing_gt>=1))
```


So, if we exclude individuals with 8/9 successfully scored loci from re-runs we substantially reduce the number of re-run work we need to do.

Require 9 scored loci:
~ 4 96-well plates panel 1
~ 3 96-well plates panel 2
~ 2 96-well plates panel 3

Require 8 scored loci:
~ 2 96 well plates at each panel , almost all overlap, suggesting that we could substantially speed up work by using the same plate layout across all panels. If we took this approach, of the 2043 non-sex marker missing genotypes, we would regenotype 1067. This also wouldnt "waste" that many resources. Of the 594 panel re-runs from the 198 individuals with 3 or more missing genotypes total, only 114 have no missing data. If we choose to run 96 samples per plate (no negative controls) and use 2 plates, then we are 6 samples short. Let's be sure to pick these 6 that have 8, not 7 total loci genotyped.


## Final Decision

Went with the "require 8 scored loci" approach described above. Almost every sample that had 3 or more missing genotypes was extracted again from fresh tissue and run at all panels. 

Also re-run ALL individuals that failed at the sex marker.


__Current plan (lab work perspective)__

cherry pick tubes and set up plate layout for 96*2 samples
Extract DNA
amplify at panels 1, 2, and 3 
coload 
ABI run - (6 96-well abi runs)

Pull ALL individuals with failed sex genotypes, re-extract those that aren't in the panel-1-3 re-runs and regenotype

__Current plan (all work perspective)__

find overlap of failed sex marker inds, and failed panels 1-3 inds
to save work, make sure the plate layouts overlap for these individuals ()

```{r}
failed_sex_marker_inds <- gt_l_0.1 %>%
  filter(marker == "OtY3", is.na(a1)) %>%
  pull(sample)

sum(failed_sex_marker_inds %in% missing_at_least_3_overall)
```

__Sample Lists__

Let's put together the sample lists for re-runs

```{r}
# first eliminate 6 individuals from the missing_at_least 3 overall, but make sure they are ones with 8, not 7 genotypes
rr_inds <- missing_count %>%
  slice_max(order_by = missing_gt, n = 192, with_ties = FALSE) %>%
  pull(sample)

rerun <- as.data.frame(c(rr_inds, failed_sex_marker_inds))
colnames(rerun) <- "sample"

rerun %<>%
  distinct(sample) %>%
  left_join(missing_count) %>%
  mutate(missing_sex_marker = case_when(sample %in% failed_sex_marker_inds ~ TRUE,
                                        TRUE ~ FALSE)) %>%
  arrange(desc(missing_sex_marker), sample)

  
```

Now let's get the container information and add to this to make plating easier.

```{r, message=FALSE, warning=FALSE}
prog <- readxl::read_xlsx("../metadata/dayan_metadata/processed_metadata/Dayan_Oct2021_progeny_export.xlsx", sheet = 1, guess_max = 10000 )  

prog %<>%
  select(sample = `Sample Name`, `Container Name`)

rerun %<>%
  left_join(prog )

rerun %<>%
  mutate(plate_group = case_when(sample %in% rr_inds ~ "plate29_30",
                           TRUE ~ "plate_31"))

# some of these are listed as batch samples in the version of the metadata available here. will manually edit them in the output spreadsheet

#write_tsv(rerun, "../lab_work/genotyping/re_run_planning/two_plate_rerun_list.txt")
```


# First Round Re-Run

Let's import the genotypes from the first (hopefully only) re-runs, and evaluate them.

```{r}

#import
gm_rerun_a <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_29_30 Genotypes Table.txt", col_types = cols(.default = "c"))
gm_rerun_b <- read_tsv("genotype_data/genemapper_output/MCKR_Plate_31_29 Genotypes Table.txt", col_types = cols(.default = "c"))

gm_rerun <- bind_rows(gm_rerun_a, gm_rerun_b)

# remove blanks and controls
gm_rerun %<>%
  filter(str_starts(`Sample Name`, "Ots"))


# sorta long (each allele is its own column for each marker/individual)
gt_rerun_l_0.1 <- gm_rerun %>%
  select(sample = `Sample Name`, marker = Marker, panel = Panel, a1 = `Allele 1`, a2 = `Allele 2`)

# super long (for conversion only, totally long - each allele/marker/individual is its own row)
gt_rerun_0.1_sl <- gm_rerun %>%
  select(sample = `Sample Name`, marker = Marker, panel = Panel, a1 = `Allele 1`, a2 = `Allele 2`) %>%
  pivot_longer(cols = c(a1, a2), values_to = "allele")

#now convert to wide format (each marker gets two columns, format for colony and cervus)
gt_rerun_0.1 <- gt_rerun_0.1_sl %>%
  pivot_wider(id_cols = c(sample), names_from = c(marker, name), names_prefix = "Ot",  values_from = allele)
```

## Missingness

How did the reruns go?

```{r}
miss_rerun <- gt_rerun_l_0.1 %>% 
  filter(marker != "OtY3") %>%
  group_by(sample) %>%
  count(a1) %>%
  filter(is.na(a1)) %>%
  rename(n_missing = n) %>% 
  right_join(distinct(gt_rerun_l_0.1, sample)) %>%
  filter(sample %in% gm_rerun_a$`Sample Name`) %>%
  mutate(n_missing = case_when(is.na(n_missing) ~ 0,
                               TRUE ~ as.numeric(n_missing))) %>%
  mutate(carcass = case_when(str_starts(sample, "OtsAC") ~ "non-carcass",
                             TRUE ~ "carcass"))
miss_rerun %<>%
  ungroup()

ggplot(data = miss_rerun)+geom_histogram(aes(x = n_missing, fill = carcass), bins = 11, position = "dodge")

miss_rerun %>%
  count( carcass, n_missing)
```

Of the 192 samples re-run from tissue at all panels, 140 were fresh tissue and 52 were carcass samples. 

For the fresh tissue samples, 90 (64%) were successfully genotyped at 8 or more markers and 94 (67%) were genotyped at 7 or more. This suggests that re-runs were pretty successful, and when combined with previous data, our dataset will be much more complete.

For the carcass samples, 22 (42%) were successfully genotyped at 8 or more loci.

Looking at the missingness distribution for re-runs and comparing to the first round genotyping, I suspect additional re-runs will provide very quickly diminishing returns; missingness rates are worse for re-runs made from fresh extractions than initial genotyping, suggesting that tissue quality is contributing to missingness.

__Let's call it, NO MORE RERUNS__

# Re-Run Incorporation

In this section we will update the gt_0.1 dataset to include info from re-runs.

```{r}

gt_rerun_l_0.1 %<>%
  rename(a1_rr = a1, a2_rr = a2)

gt_l_0.2 <- gt_l_0.1 %>%
  left_join(gt_rerun_l_0.1)
```

Now we can coalesce the two calls, but first, what should we do when an individual was genotyped twice at the same marker?

To decide, we should know how often do called genotypes disagree, this is also a possibly useful estimate of overall genotype error rate, but probably an overestimate

## Error Rate

```{r}
#
error_rate_dataset <- gt_l_0.2 %>%
  filter(sample %in% gm_rerun_a$`Sample Name`) %>%
  filter(!(is.na(a1)), !(is.na(a1_rr))) %>%
  mutate(a1_agree = (a1 == a1_rr) | (a1==a2_rr),
         a2_agree = (a2 == a1_rr) | (a2==a2_rr)) %>%
  left_join(select(miss_rerun, -a1)) %>%
  left_join(missing_count)
```

Wow, about 9%. This is potentially very bad because of the single mismatch tolerance in the assignment protocol. It's also way higher than the number we use as the prior in COLONY and CERVUS and higher than the error rate calculated from regenotyping about 1% of all samples in the previous work (~2% from 2007-2013 data)

However, it could be that by focusing on the worst individuals (the ones needing re-runs) we're inflating the error rate. If this was the case there should be a relationship between probability of genotypes within an individual not agreeing and the number of missing genotypes for that individuals. Let's check before getting too worried here. 


```{r}

error2 <- error_rate_dataset %>%
  group_by(sample) %>%
  summarise(disagree_rate = sum(a1_agree == FALSE), n_missing = n_missing, missing_gt = missing_gt)

ggplot(data = error2)+geom_smooth(aes(n_missing, disagree_rate/11), method = "lm")+ggtitle("gt disagree rate vs rerun missingness rate")
```

Woof okay what a relief, the error rate is much closer to expected for samples with 3 or fewer missing genotypes that will be retained in the analysis.

Let's check just to be sure. When only examining individuals that will be retained in the analysis (at least 7 loci genotyped)

```{r}
error2 %>% 
  ungroup() %>%
  filter(missing_gt < 4) %>%
  summarise(mean_error = mean(disagree_rate)/11)
  
```

Nice!!! The mean error rate is 1.6%, very close to the average error rate from previous data.

```{r, eval = FALSE}
ggplot(data = error_rate_dataset)+geom_density(aes(x = n_missing, fill = a1_agree), alpha = 0.2)

```

## Incorporate Re-Runs

We've also learned that there is a relationship between missingness rate and genotype accuracy, so perhaps this is the best way to break ties, use whatever came from the run with the least overal missing data. Forturnately the re-run almost always had fewer missing data than the original (makes sense), so let's just defer to the re-run genotype when there is overlap.

```{r}
# we can assign priority to the re-run by putting it first in a coalesce() call in dplyr

gt_l_0.3 <- gt_l_0.2 %>%
  rename(a1_1 = a1, a2_1 = a2) %>%
  mutate(a1 = coalesce(a1_rr, a1_1),
         a2 = coalesce(a2_rr, a2_1)) %>% # View(.) check if this worked here
  select(sample_id = sample, marker, panel, a1, a2)
```


## Missingness

Now that we've added our re-runs how does the dataset look?

First how many individuals are above the 7 locus threshold for retention in the analysis?
```{r}
gt_l_0.3 %>%
  filter(marker != "OtY3") %>%
  group_by(sample_id) %>%
  summarise(n_gt = 11-sum(is.na(a1))) %>%
  count(n_gt) %>% 
  mutate(perc = n/2580) %>%arrange(desc(n_gt))
```

98.2% of individuals (2536 / 2580) are genotyped at 7 or more microsatellite loci. Of the 140 individuals that failed to meet this threshold after initial genotyping, re-runs were able to recover 96 of these!

Let's plot missingness.

```{r}
ggplot(gt_l_0.3 %>%
  filter(marker != "OtY3") %>%
  group_by(sample_id) %>%
  summarise(n_gt = 11-sum(is.na(a1))) %>%
  count(n_gt) %>% 
  mutate(perc = n/2580) %>%arrange(desc(n_gt))) + geom_bar(aes(x = n_gt, y = n ), stat = "identity")
```

How many of the ~2% of excluded samples are carcass samples?

```{r}
gt_l_0.3 %>%
  filter(marker != "OtY3") %>%
  group_by(sample_id) %>%
  summarise(n_gt = 11-sum(is.na(a1))) %>%
  mutate(carcass = case_when(str_starts(sample_id, "OtsAC")~"live",
                             TRUE ~ "carcass")) %>%
  count(carcass, n_gt) 
```

Of the 44 exlcuded samples 17 are carcasses, and 27 are live tissue.

## Save

Let's save some files at this point. 

```{r}

# super long (for conversion only, totally long - each allele/marker/individual is its own row)
gt_0.3_sl <- gt_l_0.3 %>%
  pivot_longer(cols = c(a1, a2), values_to = "allele")


gt_0.3 <- gt_0.3_sl %>%
  pivot_wider(id_cols = c(sample_id), names_from = c(marker, name), names_prefix = "Ot",  values_from = allele) %>%  
  select(sample_id, starts_with("Ot201"),starts_with("Ot209"),starts_with("Ot249"),starts_with("Ot253"),starts_with("Ot215"), starts_with("Ot311"),starts_with("Ot409"),starts_with("Ot211"),starts_with("Ot208"),starts_with("Ot212"), starts_with("Ot515"), starts_with("OtOtY"))
```

```{r, eval = FALSE}
save(gt_0.3, file= "genotype_data/gt_0.3.RData")
save(gt_l_0.3,file =  "genotype_data/gt_l_0.3.RData")
write_tsv(gt_0.3, file = "genotype_data/gt_0.3.txt")
write_tsv(gt_l_0.3, file = "genotype_data/gt_l_0.3.txt")

```



# Prep input dataset

Here we want to consolidate the genetic data for all individuals in the metadata.

## Genetic Data Sources

__data sources__  
Genetic data comes from four sources: the final genetic dataset from Dayan (gt_0.3), a file from Nick Sard confirmed to be the exact file used as input for the 2016 tech report and manuscript pedigress (10APR2016_Cougar_adult), an additional file from Nick for carcass samples (04Jan2016_below_dam_Cougar_adult), and a file consolidated by Vickie Zeller in 2017 (progeny_microsats.txt) 

Where they overlap (2007-2014), the 2017 Zeller dataset has slightly different genetic data with fewer missing data and a very small number of different genotype calls than the Sard files. These genotypes ARE NOT the ones used in the 2016 manuscript and tech report. However this file is the only source of genetic information for 2016 and 2017 Cougar Trap samples, so we use it there. Still searching for an explanation of why these files are different. The smaller number of uncalled genotypes suggests re-runs were completed to fill in missing data, but this is probably not the case because the only files that have matching genotypes come from a directory used for the RRS calculation in the 2014 USACE tech report (Directories_Shared_by_Nick/2014/Cougar Dam Tech Report/RRS_final) which is dated BEFORE any analyses for the 2016 tech report. Everywhere else uses the genotypes that match 10APR2016_Cougar_adult, therefore I suspect these Zeller genotypes are from a less conservative round of genotype scoring, and that the final genotypes were never uploaded to progeny. I'm still still looking for evidence to support this explanation. Unfortunately a progeny crash wiped out when and who uploaded genotypes to progeny.

## Merge 

First let's load the data we'll need into R. 

```{r}
load("../metadata/dayan_metadata/processed_metadata/metadata.Rdata")

sard_genos <- readxl::read_xlsx("genotype_data/old_genotypes/10APR2016_Cougar_adult_master.xlsx", sheet = 1, col_types = "text")

prog_micro <- read_tsv("genotype_data/old_genotypes/Progeny_microsats.txt", col_types = cols(.default = "c"))

sard_sgs_raw <- read_tsv("../metadata/prev_metadata/04JAN2016_below_dam_Cougar_adult_master.txt", col_types = cols(.default = "c"))

id_key <- read_tsv("genotype_data/old_genotypes/id_key.txt")

```

Let's unify the column names for merging. 

```{r}
# let's convert to the dayan datasets column naming convention


# let's also fix the naming convention mistake for 2018 cougar trap samples (named OtsAC18SFML in metadata but OtsAC18MCKR in genotype data)

gt_0.4 <- gt_0.3 %>%
  rename(OtY3_a1 = OtOtY3_a1, OtY3_a2 = OtOtY3_a2) %>%
  mutate(sample_id = case_when(str_starts(sample_id, "OtsAC18MCKR") ~ str_replace(sample_id, pattern = "MCKR", replacement = "SFMK"),
                               TRUE ~ sample_id))

prog_micro %<>%
  select(sample_id = `Progeny name`, 
         Ot201_a1 = Ots201b, Ot201_a2 = Ots201b_1,
         Ot209_a1 = Ots209, Ot209_a2 = Ots209_1,
         Ot249_a1 = Ots249, Ot249_a2 = Ots249_1,
         Ot253b_a1 = Ots253b, Ot253b_a2 = Ots253b_1,
         Ot215_a1 = Ots215, Ot215_a2 = Ots215_1,
         Ot311_a1 = OtsG311, Ot311_a2 = OtsG311_1,
         Ot409_a1 = OtsG409, Ot409_a2 = OtsG409_1,
         Ot211_a1 = Ots211, Ot211_a2 = Ots211_1, 
         Ot208_a1 = Ots208b, Ot208_a2 = Ots208b_1,
         Ot212_a1 = Ots212, Ot212_a2 = Ots212_1,
         Ot515_a1 = Ots515, Ot515_a2 = Ots515_1)

sard_sgs_raw %<>%
  select(sample_id = `Sample Name`, 
         Ot201_a1 = OT201, Ot201_a2 = OT201_1,
         Ot209_a1 = OT209, Ot209_a2 = OT209_1,
         Ot249_a1 = OT249, Ot249_a2 = OT249_1,
         Ot253b_a1 = Ot253b, Ot253b_a2 = Ot253b_1,
         Ot215_a1 = Ot215, Ot215_a2 = Ot215_1,
         Ot311_a1 = Ot311, Ot311_a2 = Ot311_1,
         Ot409_a1 = Ot409, Ot409_a2 = Ot409_1,
         Ot211_a1 = Ot211, Ot211_a2 = Ot211_1, 
         Ot208_a1 = Ot208, Ot208_a2 = Ot208_1,
         Ot212_a1 = Ot212, Ot212_a2 = Ot212_1,
         Ot515_a1 = Ot515, Ot515_a2 = Ot515_1)

sard_genos %<>%
  select(hmsc_id = `Sample.Name`, 
         Ot201_a1 = OT201...6, Ot201_a2 = OT201...7,
         Ot209_a1 = OT209...8, Ot209_a2 = OT209...9,
         Ot249_a1 = OT249...10, Ot249_a2 = OT249...11,
         Ot253b_a1 = Ot253b...12, Ot253b_a2 = Ot253b...13,
         Ot215_a1 = Ot215...14, Ot215_a2 = Ot215...15,
         Ot311_a1 = Ot311...16, Ot311_a2 = Ot311...17,
         Ot409_a1 = Ot409...18, Ot409_a2 = Ot409...19,
         Ot211_a1 = Ot211...20, Ot211_a2 = Ot211...21, 
         Ot208_a1 = Ot208...22, Ot208_a2 = Ot208...23,
         Ot212_a1 = Ot212...24, Ot212_a2 = Ot212...25,
         Ot515_a1 = Ot515...26, Ot515_a2 = Ot515...27)

# add new ids to sard data, note that we don't have these keys for the sard sgs samples

sard_genos %<>%
  left_join(id_key) %>%
  relocate(sample_id)

# now let's remove the rows from the prog_microsats file we don't need
prog2 <- prog_micro %>%
  filter(!(sample_id %in% sard_genos$sample_id))

#merge into single dataframe
gt_0.5 <- bind_rows(gt_0.4, sard_genos, sard_sgs_raw, prog2)

```


__Merge__
Now let's merge with the metadata.

First, did any samples in the metadata fail to get the genos attached? We know from the meta_data_consolidation notebook there should be some issues here.
```{r}
full_data <- metadata %>%
  left_join(gt_0.5, by = c("sample_id" = "sample_id"))

# let's start the work of cleaning this up

# first, did any samples in the metadata fail to get the genos attached
full_data %>%
 filter(across(starts_with("Ot"), ~ is.na(.))) %>% View(.)

```

Only the ones we expected to fail are missing, great work! We'll just make a file that links these manually.

Listing these here to answer any future questions:  
9 NA sample names, these are individuals that are known to exist from the raw, source metadata, but there was no fin clip taken (8 sample), or they were removed from the analysis (couldn't reliably link metadata to fin clip, 1 sample). Consequently given no name.  

4 2016 carcass samples: OtsCC16MCKR_1100, OtsCC16MCKR_1161, OtsCC16MCKR_1162, OtsCC16MCKR_1243 -  These got skipped during first round genotyping when I didn't understand how to parse the different sets of "bridge to mouth" carcass samples. They were included during re-runs  

Hatchery outplant HORs after 2017: These are neither potential parents nor potential offspring in this study and are not genotyped.  

OtsAC14MCKR_0344: Sample given a name in metadata and a labeled tube supplied by ODFW, but the tube was totally empty, no ethanol, tag or fin clip.

Now let's add these genotypes to the full_data dataframe.
```{r}
miss_a <- read_tsv("../genotypes/genotype_data/missing_genos.txt", col_types = cols(.default = "c"))
miss_carc <-  read_tsv("../genotypes/genotype_data/missing_genos_carcass.txt", col_types = cols(.default = "c"))

miss_genos <- bind_rows(miss_a, miss_carc)
  
```

```{r}
# sadly there is no coalescing join function yet in dplyr, let's borrow someone else's work 

coalesce_join <- function(x, y, 
                          by = NULL, suffix = c(".x", ".y"), 
                          join = dplyr::full_join, ...) {
    joined <- join(x, y, by = by, suffix = suffix, ...)
    # names of desired output
    cols <- union(names(x), names(y))
    
    to_coalesce <- names(joined)[!names(joined) %in% cols]
    suffix_used <- suffix[ifelse(endsWith(to_coalesce, suffix[1]), 1, 2)]
    # remove suffixes and deduplicate
    to_coalesce <- unique(substr(
        to_coalesce, 
        1, 
        nchar(to_coalesce) - nchar(suffix_used)
    ))
    
    coalesced <- purrr::map_dfc(to_coalesce, ~dplyr::coalesce(
        joined[[paste0(.x, suffix[1])]], 
        joined[[paste0(.x, suffix[2])]]
    ))
    names(coalesced) <- to_coalesce
    
    dplyr::bind_cols(joined, coalesced)[cols]
}

full_data_0.2 <- coalesce_join(full_data, miss_genos, by = "sample_id")
```

__Sex__
Next up is cleaning up the geno_sex column and consolidating into the final sex determination used for the study. 

In the study, we will use genotypic sex, if available. If not then we will use phenotypic sex. If phenotypic sex is also missing, we will call sex NA, fortunately only a single sample has NA sex and it also is not genotyped at enough loci to make it to the final dataset, so all samples used for pedigree analysis are sexed!

```{r}
# now the geno_sex and OtY3 mess
# for the new samples there is no geno sex in the metadata, let's fix this
full_data_0.3 <- full_data_0.2 %>%
  mutate(geno_sex = case_when(is.na(OtY3_a1) ~ geno_sex,
                              OtY3_a2 == "M" ~ "M",
                              OtY3_a2 == "308" ~ "F",
                              TRUE ~ NA_character_),
         sex = coalesce(geno_sex, pheno_sex),
         sex = case_when(sex == "J" ~ "M",
                         TRUE ~ sex)) %>% #call jacks males for the final sex variable
  relocate(sex, .before = pheno_sex)


```


__Column Names__

Let's also clean up the column names

```{r}
full_data_0.3 %<>%
  mutate(old_id = coalesce(hmsc_id.x, hmsc_id.y)) %>%
  select(- c(starts_with("hmsc"), starts_with("OtY"))) %>%
  relocate(old_id, .after = sample_id)
  
```


__Unify Missing Data__

Missing genotypes are scored many different ways across the datasets. In both Colony and Cervus, missing data must be coded as "0"

```{r}
full_data_0.3 %<>%
  mutate(across(starts_with("Ot"), ~ case_when(. == "000" ~ "0",
                                               is.na(.) ~ "0",
                                               TRUE ~ (.)))) 
```


# Final Unfiltered Dataset Summary

Here we summarise the final dataset (before filtering for missing data) and conduct a final QC check against published metadata. 

```{r}
require(kableExtra)
require(gt)
require(gtsummary)


distinct_full <- full_data_0.3 %>%
  distinct(sample_id, .keep_all = TRUE) %>%
  mutate(year = as.factor(year))




tbl_summary(select(distinct_full, type, year, origin), by = type) %>%
  modify_header(label ~ "") %>%
  modify_spanning_header(c("stat_1", "stat_2", "stat_3", "stat_4") ~ "**type**") %>%
  as_kable_extra() %>%
  kable_classic(full_width = F, html_font = "Arial")# %>%


```

## Discrepancies

Here we explain any remaining discrepancies in sample sizes with previously published results (Sard 2016 and Banks 2016). 

### 2009 
Hatchery Outplants 2009: In table 1 from Sard 2016 there are 1347 hatchery outplants from 2009, but 1386 total samples in 2009, despite that there are no other samples in 2009. I believe this is simply a little unclear in the manuscript, nothing is actually wrong. The issue stems from the fact that in 2009, 39 hatchery outplants were from Leaburg Hatchery, while 1347 were from McKenzie Hatchery. 

### 2011  
__Hatchery Outplants:__ In table 1 from Sard 2016 there are 344 hatchery outplants from 2011, but there are 345 in my data and in Nick's final dataset (10APR2016_Cougar_adult_master). This isn't explained by filtering, there are 339 hatchery outplants actually considered as potential parents after filtering. There are no files in all of Nick's directories with 344 hatchery outplants from 2011. Ultimately cannot provide an explanation for this discrepancy nor can I fix it.

__Cougar Trap 2011:__ In table 1 from Sard 2016 there are 387 Cougar trap samples including 30 HORs and 257 NORs from 2011, but there are 386 (29 HOR + 257 NOR) in my data and in Nick's final dataset (10APR2016_Cougar_adult_master).  There are no files in all of Nick's directories with 387 Cougar trap samples from 2011. Perhaps they were counted in excel and someone included the header row? I can't explain where this extra fish comes from.

__Explanations?__
If things were counted using excel it's easy to make this mistake (one missing from one group and one extra in the other) since one group follows the other (hatchery outplants and HOR Cougar trap), so it's possible one fish got counted in the wrong category, (since the total is the same 731) but I can't demonstrate this. Ultimately we can't explain this discrepancy with 100% confidence.

### 2014  
The June 2016 tech report (Banks 2016) has VERY different samples size than my metadata for 2014 at both hatchery outplants and cougar trap samples. The draft of the USACE proposal (before I got to it) for this work also had VERY different numbers from my metadata. 

__Hatchery Outplants:__

The discrepancy with the proposal is resolved. The proposal stated there were 683 hatchery outplants. This is an easy mistake to make if you are unfamililar with how the Cougar trap metadata recycling program is recorded in the raw spreadsheets or that the Forest Glen Release site is below the dam, not above. Whoever put these numbers together combined all NORs released from the trap according to progeny. If only the first instance of an individual is recorded (how the data is recorded in progeny) (including those released downstream that never returned) there appear to be 192 NOR trap fish released, combined with the 491 hatchery outplants above for a total of 683. 

The discrepancy with the Banks report is also explainable, but not with 100% confidence. Here the 491 hatchery outplants released above cougar dam were potentially combined with (a) the 70 hatchery outplants released above trailbridge dam on the mainstem (not part of this study, but labeled as outplants when the tissues come from ODFW) and (b) the 128 NOR cougar trap individuals released above the dam according to the Sard datasets. This adds up, but it seems like a surprising mistake to make, given that these folks are familiar with the system. It's an an easy mistake to miss in review though, because they never deal with hatchery outplants in the report (2014 is not a parent year).

In any case, I pulled any individual with a OtsAC14MCKR or OtsAC14SFMK pedigree out of the tissue archive, searched the boxes for the original intake forms and physically accounted for all tissue samples from 2014. I found 561 hatchery outplants, with 70 released above trailbridge dam for a total of 491 in this study. This matches the progeny data.

__Cougar trap__

The Banks report (2016, Table 1) states that 193 NOR Cougar trap samples are released above the Dam. My table above states there are 214 total Cougar trap samples, regardless of origin or where they are released, so really there's no discrepancy because they are counting different individuals (NOR vs total), but let's dig a little deeper here, because some confusing things are going on.  

First, 193 NOR trap individuals are not released above the dam as stated in the report, this is an error. 193 seems to arise from the 192 NOR trap samples (regardless of release site) in Nicks's dataaset, plus the single individual with no metadata (named OtsAC14SFMK_0120 / MRA14_0120 / OtsAC14SFMK_0 depending on the dataset), but the data carries the same error as described before: Forest Glen is below the dam, and some of the fish that are recycled (at Forest Glen) never return to be reintroduced above the dam. After accounting for this only 133 NOR Cougar Trap samples are released above the dam.

Here I spent a lot of time to be confident that my version of the metadata is correct. I went back to the original spreadsheets from the Cougar Trap provided by USACE and parsed them (see cougar_trap_metadata_mgmt notebook). Much like the mistake described for 2014 hatchery outplants, this is an easy one to miss because the 2014 report never deals with where these fish are released, they are only interested in them as offspring. My metadata and the previous dataset agree on the total number of unique individuals that are considered as offspring from this year (192). 

__SGS__  
There is an apparent discrepancy in the number of NOR SGS in my metadata and what is presented in the Banks 2016 report (table 2). In the table only 21 carcass samples are listed in 2014, this corresponds to the number of NOR carcass samples between the confluence with the mainstem and Cougar Dam . This is correct as far as I can tell.

My data has 27 listed in the table above, this is because I also include above dam carcass samples as potential parents (5), and because of a sgs field crew error where the same individual id was used for two fish (3039a -the last fish of one work day and 3039b - the frst fish of the next work day). I split these up but the previous report counted chose to ignore one.

Another potential area for confusion here: While the table states only 21 samples were taken from carcasses on the South Fork below cougar in 2014,  all of the input datasets for colony and cervus (and raw output pedigrees) contain 75 2014 carcass samples as potential offspring. This corresponds to the total number of NOR carcasses sampled anywhere on the McKenzie that passed the filtering threshold used for that year (tolerate only 2 missing genotypes, remove duplicates), not just on the south fork from South Fork to the Dam. It would appear that these were included as potential offspring in the pedigree inference, but none were assigned to a parent.



### 2015 
__Hatchery outplants__   
My dataset contains 600 hatchery outplants. Table 1 in Banks 2016 states there are 619 HOR individuals outplanted above the dam. Again there is no discrepancy here, we're counting different things. Table 1 from Banks 2016 considers both the 600 hatchery outplants and the 19 HOR individuals from the Cougar trap that were released above the dam.

__Cougar Trap__   
Table 1 from the Banks report states that 241 NOR individuals were outplanted above the dam. This is a similar mistake to before, there are 241 unique NOR individuals sampled at the Cougar Trap. However, all of these fish were recycled downstream to Forest Glen or released into the tailrace on their first capture. Only 136 returned and were outplanted above the Dam. Since the previous report was unconcerned with where these fish eventually wound up, it was an easy mistake to make.  

__SGS__
Banks 2016 tech report table 2 states 55 NOR carcass samples below. My metatdata agrees and includes two additional samples from above the dam.

# Filtered Dataset

## Rationale

The filtering approach used in the previous reports varies from colony/cervus run to run and year to year. In some cases no filtering is done prior to assignment in Cervus and Colony, in others very strict filtering is used (duplicates removed and individuals missing 2 or more loci removed) prior to assignment. The transition occurs at 2014 (offspring year), when the 2014 and 2015 offspring years were added for the June 2016 tech report. This is reflected in the change in Sard's gt.prep scripts. Return years 2010 to 2013 use the gt.prep script, but 2014 and 2015 each use their own prep scripts (e.g. gt.prep.2014) that follow the much more strict >= 9 genotyped loci filtering cutoff and removal of potential duplicate samples. 

To be clear, for 2010 - 2013 offspring years, Colony and Cervus input files are not filtered. There was an internal filtering step of >=6 loci required before assignment in Cervus. No filtering whatsoever for Colony. However, very few individuals would be filtered for missingness in this dataset anyway, so the pedigree should not very much from one based on filtered datasets. For 2014 and 2015 individuals with 2 or more missing loci and potential duplicates were filtered.

A challenge here is that in the previous analyses, cervus was run by comparing a single parent year to a single offpsring year, then all possible pedigrees for a given offspring year were concatenated. This is not acceptable going forward as it creates many problems (discussed elsewhere). Instead, all possible parents for a given offspring must be considered simultaneously by cervus and colony. However, now we have a set of new problems; there is no way to filter the dataset in the same manner as before while also (1) fixing the problem described above because the different filtering parameters create different sets of parents from run to run that will now be combined into a single run, and (2) creating a single dataset that can be applied (and summarised) analysis wide.

So what to do here??? 

Going forward, I chose to apply a single filtering scheme across the entire dataset.

In the North Santiam system the filtering cutoff of 7 scored loci was used, based on the non-exclusion probabilities calculated in CERVUS. Using the non-exclusion probabilities published in the supplemnt to the North Santiam manuscript, if we choose the 7 worse loci, this corresponds to a non-exclusion probability of 0.00019 for the first parent, 4.56E-06 for the second parent and 5.4E-10 for a parent pair.  

>Note that in the paper, the authors added these up sequentially in order by name (instead of ordered by worse exclusion power) for non-exclusion probs of 6.85E-05	for the first parent, 1.47E-06 for the second parent and 6.42E-11 for a parent pair. This is not as conservative as it could be. Another anti-conservative decision made in this paper is that all data were combined across all years. This inflates the diversity relative to what is used to actually run cervus (offspring year by offspring year) and makes for more liberal estimates of the non-exclusion probability.  

The approach in the 2016 sard paper was different, they avoided the inflation of diversity problem by calculating the non-exclusion probabilities for each dataset, but ran into a different problem: they didn't account for missing data and present the probabilities assuming all 11 loci are scored. They explain their rationale here with a comment to the effect of "only a tiny portion of individuals weren't genotyped at 10 or 11 loci, so any effect is probably small." This seems fair, but isn't as defensible if the missing data rate (or filtering cutoff) is less strict. 

My solution is to apply the North Santiam cutoff to have a consistent dataset across all offspring years (7 loci required before inclusion in pedigree analysis), but to report the expected non-exclusion probability and associated expected number of false parent-offspring pairs for each dataset as a data supplement. This seems to be the best of both worlds, but will create a little bit of extra work.  

## Missingness Filter

Finally, let's apply the missingness filter (individuals must be genotyped at at least 7 of 11 loci)

```{r}
full_data_0.4 <- full_data_0.3 %>%
  mutate(miss_gt = rowSums(across(starts_with("Ot"), ~ .x == "0"))/2)
                           
full_data_0.4 %<>%
  filter(miss_gt <=4)
```

Let's create a table of who was removed, this will likely need to be summarised in the report.

```{r}
miss_filt <- full_data_0.3 %>%
  mutate(miss_gt = rowSums(across(starts_with("Ot"), ~ .x == "0"))/2) %>%
  filter(!(miss_gt <=4))

kable(miss_filt %>%
  count(year, type, origin, sex), caption = "number of individuals filtered from the full dataset due to missingness (fewer than 7 scored loci)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

Most are SGS samples, no surprise there, but something to note is that the 2019 Cougar Trap samples have a lot of bad samples. We already knew this because they made up a lot of the re-runs and a lot still failed even with freshly extracted DNA. Something likely went wrong with tissue sampling or storage this year. 

## Duplicate Filter

Here we remove any samples that are potentially from the same individual. There are several ways the same individual could be genotyped more than once: (1) carcass tissue sample taken from NOR cougar trap sample released downstream, (2) batch sampling leads to one fin clip being split into two, and then being treated as two separate samples, (3) above dam carcass sample from an individual released upstream

In the North Santiam paper/reports this method is explained:

> Sets of identical genotypes were assumed to represent a single individual and the duplicates removed from the dataset. Genotypes that matched at all but one locus were also removed, as the probability of observing a near-identical multilocus genotype at 10 of the microsatellite loci used in this study was extremely low (< 110-20) in each year, as calculated in GenAlEx (Peakall and Smouse 2012). 

In the McKenzie paper and reports the duplicate filtering is not described, and we know that it was only applied to some input datasets for parentage and not others. We do have the list of duplicates by offspring year through the "gts.duplicated" files. From these files I inferred that only perfect matches were considered duplicates.

We'll take the same approach (perfect matches are considered duplicates)

```{r}
duplicates <- full_data_0.4 %>%
  distinct(sample_id, .keep_all = TRUE) %>% #some individuals are in there mutliple times on purpose
  group_by(year, across(starts_with("Ot"))) %>%
  filter(n()>1) %>% 
  arrange(year, across(starts_with("Ot")))

```

19 pairs of individuals within years have perfectly matching genotypes.

Of the 19 pairs of individuals with perfectly matching genotypes, 5 seem more likely to be sibs than duplicates. For example OtsAC09SFMK_0167 and OtsAC09SFMK_0168 are released on different days at different sites. How could this possibly be the same fish without an error in the metadata? Similarly, MR12TH_227 and MR12TH_230 are both cougar trap samples released at Hard Rock on the same day, but they had recorded lengths that differed by 90mm. Seems more likely they are sibs than the same fish.

More frequently (14 of 19), the pair is easily explained, either they are from a batch sample jar (e.g. 2016 hatchery outplants), or they are from a fish that was sampled, released and then likely sampled again as a carcass.

I think we should filter these individuals out, regardless of whether we think they are duplicates or not. Later, in the pedigree assignment script, perfectly tied parentages are chosen by taking whichever occurs first in the dataset. So it the filtering will happen at that point anyway. Describing it as a filtering step seems more transparent for the readers/reviewers than having to get into the weeds of the CERVUS/COLONY consensus assignment algorithm. 

I wrote these out to file, because they are likely going to of interest. For example, if fish are recycled downstream and don't return, this is taken as evidence that they might have been strays. However, if we demonstrate that many simply die on their attempt to reach the trap a second time, it suggests the recycling program is potentially harming NORs trying to return to above dam habitat. 

```{r, eval = FALSE}
write_tsv(duplicates, file = "./genotype_data/duplicates.txt")
```

Let's do the filtering. If one sample is a carcass sample, we keep the live fish.

```{r}
dups_to_remove <- duplicates %>%
  rowid_to_column(var = "row_id") %>%
  filter(row_id %% 2 == 0) %>% # the carcass sample is always the second in the pair with a live sample because of the way we arranged the data, so we can just take the modulo of the row number to get the right individuals from the pairs to filter
  pull(sample_id)

full_data_1.0 <- full_data_0.4 %>%
  filter(!(sample_id %in% dups_to_remove))
```

Let's also create a table describing who was removed.
```{r}
kable(duplicates %>%
        ungroup() %>%
  count(year, type, origin, sex), caption = "number of individuals filtered from the full dataset due duplication (all scored loci match)") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```


# Save Final Datasets

Let's save these as r objects, tsv and excel spreadsheets.


We would like to save the final unfiltered dataset and the final filtered dataset under clear, new names. We also need some files with ONLY genotype data to subset for cervus and colony inputs, this file also no longer needs multiple rows per fish (from fish that were sampled twice)

```{r, eval = FALSE}
# unfiltered (no missingness or duplicates filtered)
full_unfilt <- full_data_0.3
save(full_unfilt, file = "genotype_data/full_unfiltered_dataset.R")
write_tsv(full_unfilt, file = "genotype_data/full_unfiltered_dataset.txt")

# fully filtered 
save(full_data_1.0, file = "genotype_data/full_filtered_dataset.R")
write_tsv(full_data_1.0, file = "genotype_data/full_filtered_dataset.txt")

# genos and sample id only (filtered only)
gt_1.0 <- full_data_1.0 %>%
  distinct(sample_id, .keep_all = TRUE) %>%
  select(sample_id, starts_with("Ot"))
save(gt_1.0, file = "genotype_data/filtered_genotypes.R")
write_tsv(gt_1.0, file = "genotype_data/filtered_genotypes.R")

```

# Cervus and Colony Output Files

Here we subset the dataset into input files for Colony and Cervus which are run per offspring year.

## Summary  

__Age Structure__
Each potential offspring is assigned to candidate parents 3, 4, 5 and 6 years prior.

__Offspring__  
All NOR individuals within a year on the South Fork McKenzie are considered potential offspring, regardless of type, release location or sampling location. This includes all NOR cougar trap samples, all SGS samples above or below the dam, and all precocial males samples above the dam.    

__Parents__  
Only individuals released above the dam are considered candidate parents. This includes hatchery outplants, NOR and HOR Cougar Trap samples, SGS samples above the dam and precocial males. We also include 3 cougar trap individuals with no recorded release site variable as these are potentially released above the dam.

## CERVUS 

Let's write a single function for Cervus that will parse the dataset and write the necesary files for a given offspring year.

Cervus requires 4 files: three lists of sample_ids (dams, sires, offspring) with each individual on a newline, and a genotype file. The genotype file is tab delimited with a single row for each individual, two columns for each loci and no other data.

```{r, eval = FALSE}

#note that I later moved the directories to the parentage directort and renamed them to CERVUS and COLONY (instead of CERVUS_inputs and COLONY_inputs)

write_cervus_files <- function(offspring_year) {
  dams <- full_data_1.0 %>% 
    filter(offspring_year - year <= 6, offspring_year - year >= 3, cand_parent == TRUE, sex == "F") %>%
    distinct(sample_id) %>%
    pull(sample_id)
  sires <- full_data_1.0 %>% 
    filter(offspring_year - year <= 6, offspring_year - year >= 3, cand_parent == TRUE, sex == "M") %>%
    distinct(sample_id) %>%
    pull(sample_id)
  offspring <- full_data_1.0 %>% 
    filter(offspring_year == year , origin == "NOR") %>%
    distinct(sample_id) %>%
    pull(sample_id)
  write_tsv(filter(select(gt_1.0, sample_id), sample_id %in% dams), file = paste0("genotype_data/CERVUS_inputs/", offspring_year, "/", offspring_year, "_dams.txt"), col_names = FALSE)
  write_tsv(filter(select(gt_1.0, sample_id), sample_id %in% sires), file = paste0("genotype_data/CERVUS_inputs/", offspring_year, "/", offspring_year, "_sires.txt"), col_names = FALSE)
  write_tsv(filter(select(gt_1.0, sample_id), sample_id %in% offspring), file = paste0("genotype_data/CERVUS_inputs/", offspring_year, "/", offspring_year, "_offspring.txt"), col_names = FALSE)
  write_tsv(filter(gt_1.0, sample_id %in% c(offspring, dams, sires)), file = paste0("genotype_data/CERVUS_inputs/", offspring_year, "/", offspring_year, "_genos.txt"), col_names = FALSE)
}
 
write_cervus_files(2010)
write_cervus_files(2011)
write_cervus_files(2012)
write_cervus_files(2013)
write_cervus_files(2014)
write_cervus_files(2015)
write_cervus_files(2016)
write_cervus_files(2017)
write_cervus_files(2018)
write_cervus_files(2019)
write_cervus_files(2020)
```


```{r, eval = FALSE}
# lets Test this for offspring year 2013, let's count who should be in these files
kable(full_data_1.0 %>%
        ungroup() %>%
        filter(year %in% c(2007, 2008, 2009, 2010)) %>%
  count(sex, type, origin, release)) %>%
  kable_classic(full_width = F, html_font = "Cambria")

count(full_data_1.0, origin, year)

# 1472 dams, YES THIS WORKED
# 2276 sires, YES THIS WORKED
# 228 offspring, YES THIS WORKED
# 3991 total genotypes, YES THIS WORKED

# also 2020, this includes both the above dam SGS and precocial males as parents, there are also parents from the recycling so there's some additional things that could break
kable(full_data_1.0 %>%
        ungroup() %>%
        filter(year %in% c(2014, 2015, 2016, 2017)) %>%
  count(sex, type, origin, release, section)) %>%
  kable_classic(full_width = F, html_font = "Cambria")

count(distinct(full_data_1.0, sample_id, .keep_all = TRUE), origin, year)

# there are three cougar samples with missing metadata included as parents, a 2014 male, a 2016 female and a 2017 male
# also note that there are a couple cougar individuals in the metadata with two above dam releases, a male in 2015 and a female in 2016
# 1624 dams in output dataset, actual number should be 1624 (1624 plus one missing meta (2016 female) minus one counted twice in the table below (2016). YES THIS LOOKS GOOD
# 1092 sires in the output dataset, should be 1091 + 2 (cougar males missing metas) - 1 (2017 male with two upstream releases) # YES THIS LOOKS GOOD
# 162 offspring YES THIS LOOKS GOOD
```

## COLONY

Colony input is a little different, we'll need to modify our function. It only takes three files, genotypes for offspring, dams and sires. 

```{r, eval = FALSE}

#note that I later moved the directories to the parentage directort and renamed them to CERVUS and COLONY (instead of CERVUS_inputs and COLONY_inputs)

write_colony_files <- function(offspring_year) {
  dams <- full_data_1.0 %>% 
    filter(offspring_year - year <= 6, offspring_year - year >= 3, cand_parent == TRUE, sex == "F") %>%
    distinct(sample_id) %>%
    pull(sample_id)
  sires <- full_data_1.0 %>% 
    filter(offspring_year - year <= 6, offspring_year - year >= 3, cand_parent == TRUE, sex == "M") %>%
    distinct(sample_id) %>%
    pull(sample_id)
  offspring <- full_data_1.0 %>% 
    filter(offspring_year == year , origin == "NOR") %>%
    distinct(sample_id) %>%
    pull(sample_id)
  write_tsv(filter(gt_1.0, sample_id %in% dams), file = paste0("genotype_data/COLONY_inputs/", offspring_year, "/", offspring_year, "_dams_genos.txt"), col_names = FALSE)
  write_tsv(filter(gt_1.0, sample_id %in% sires), file = paste0("genotype_data/COLONY_inputs/", offspring_year, "/", offspring_year, "_sires_genos.txt"), col_names = FALSE)
  write_tsv(filter(gt_1.0, sample_id %in% offspring), file = paste0("genotype_data/COLONY_inputs/", offspring_year, "/", offspring_year, "_offspring_genos.txt"), col_names = FALSE)
}

write_colony_files(2010)
write_colony_files(2011)
write_colony_files(2012)
write_colony_files(2013)
write_colony_files(2014)
write_colony_files(2015)
write_colony_files(2016)
write_colony_files(2017)
write_colony_files(2018)
write_colony_files(2019)
write_colony_files(2020)
```

